# -*- coding: utf-8 -*-
"""RAG System Using Llama2 With Hugging Face.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H293leEWKb9TJ1LRNe0aRBifXg8QlLcj
"""

!pip install pypdf

"""Transformers:

Purpose:
Enables you to build, train, and use various Natural Language Processing (NLP) models, including transformers like BERT, GPT-2, and T5.
Offers pre-trained models for a wide range of tasks, such as text classification, question answering, sentiment analysis, and summarization.

Key Features:
Modular architecture for creating custom NLP models.
Support for multiple machine learning frameworks (PyTorch, TensorFlow).
Extensive pre-trained model library covering diverse languages and tasks.

Use Cases:
Chatbots and virtual assistants.
Machine translation.
Text summarization and generation.
Text classification and sentiment analysis.
Document processing and information extraction.

einops:

Purpose:
Simplifies and improves tensor manipulation in high-dimensional data (especially PyTorch tensors).
Offers expressive syntax for Einstein summation-like operations and advanced indexing.
Key Features:
Reduces boilerplate code for common tensor operations.
Enhances clarity and readability of tensor manipulations.
Works seamlessly with existing PyTorch code and functionality.
Use Cases:
Neural network architectures that heavily rely on tensor reshaping and indexing.
Scientific computing and numerical mathematics involving tensors.
Deep learning research and experimentation with custom tensor operations.

accelerate:

Purpose:
Accelerates deep learning training on GPUs (and potentially other hardware) through mixed-precision training and gradient accumulation.
Provides a distributed training framework for multi-GPU and multi-node setups.
Key Features:
Automatic mixed-precision training for reduced memory usage and faster training.
Gradient accumulation to improve training stability and converge to better optima.
Distributed training support for scaling computations across multiple devices.
Use Cases:
Training large and complex deep learning models on a single GPU or multiple GPUs.
Optimizing training speed and resource utilization for computationally intensive models.
Large-scale deep learning projects that require distributed training across machines.

langchain:

Purpose:
Facilitates building modular NLP pipelines by chaining together transformers and other NLP components.
Streamlines the process of composing complex NLP workflows involving multiple steps.
Key Features:
Modular design for easily composing NLP tasks into pipelines.
High-level abstractions for task-specific components.
Integration with transformers and other NLP libraries.
Use Cases:
Developing multi-stage NLP pipelines for tasks like information extraction or question answering.
Designing and experimenting with custom NLP workflows.
Simplifying complex NLP projects by separating concerns and promoting code reuse.

bitsandbytes:

Purpose:
Provides specialized data structures and utilities for numerical computing, signal processing, and audio/image manipulation.
Offers efficient implementations for working with fixed-point numbers, bitwise operations, and low-level data representations.
Key Features:
Optimized data types for low-level numerical computations.
Bitwise operation support for various tasks like encoding/decoding data or implementing custom algorithms.
Tools for audio and image processing using bit-level manipulations.
Use Cases:
Signal processing and audio/image algorithms that benefit from bit-level control.
Implementations of resource-efficient neural networks (e.g., for embedded devices).
Low-level numerical computations requiring fine-grained control over data representations.
"""

!pip install -q transformers einops accelerate langchain bitsandbytes

## Embedding
!pip install install sentence_transformers

!pip install llama_index

"""1. llama_index:

VectorStoreIndex: This class helps you create and manage vector-based indices for text data. It stores text documents as dense vectors representing their semantic content, enabling fast and efficient retrieval and similarity search.
SimpleDirectoryReader: This class reads text documents from a specified directory on your file system, preparing them for indexing.
ServiceContext: This class manages configurations and settings for various services involved in the indexing process, such as language models and embedding models.
2. llama_index.llms:

HuggingFaceLLM: This class provides an interface to interact with language models hosted on Hugging Face. It allows you to send text prompts and receive responses from these models.
3. llama_index.prompts.prompts:

SimpleInputPrompt: This class represents a basic text prompt used to query a language model. It holds the text string to be sent as input to the model.
Typical Workflow with These Libraries:

Set up Service Context:
Configure necessary settings like the language model to use and embedding model for vectorization.
Load Documents:
Use a reader like SimpleDirectoryReader to load text documents from a directory.
Create Index:
Build a VectorStoreIndex by providing the loaded documents and relevant settings.
Query the Index:
Use query methods to find similar documents or retrieve relevant information based on text prompts.
Employ language models like HuggingFaceLLM to generate responses or perform other text-based tasks.
Key Considerations:

llama_index primarily focuses on text indexing and retrieval.
HuggingFaceLLM offers access to a wide range of language models on Hugging Face's platform.
For specific functionalities or tasks, you might need to import additional components from these libraries or leverage other NLP tools.
"""

from llama_index import VectorStoreIndex,SimpleDirectoryReader,ServiceContext
from llama_index.llms import HuggingFaceLLM
from llama_index.prompts.prompts import SimpleInputPrompt

import os.path
from os import path

if path.exists('/content/data') == False:
  os.mkdir('/content/data')

os.chdir('/content/data')
!pwd
!ls

documents=SimpleDirectoryReader("/content/data").load_data()
documents

system_prompt="""
You are a Q&A assistant. Your goal is to answer questions as
accurately as possible based on the instructions and context provided.
"""
## Default format supportable by LLama2
query_wrapper_prompt=SimpleInputPrompt("<|USER|>{query_str}<|ASSISTANT|>")

!huggingface-cli login

"""Instantiate HuggingFaceLLM:

This creates an instance of the HuggingFaceLLM class, connecting to the Llama-2 model hosted on Hugging Face.
Key configuration parameters:

context_window=4096: The maximum number of tokens the model can consider in a single context.

max_new_tokens=256: The maximum length of generated responses.

generate_kwargs: Controls text generation behavior (temperature=0.0 for deterministic responses, do_sample=False to avoid randomness).

system_prompt: Sets the overall task framing for the model (Q&A assistant).

query_wrapper_prompt: Formats user queries with placeholders (<|USER|>, <|ASSISTANT|>).

tokenizer_name and model_name: Specify the tokenizer and language model to use (Llama-2-7b-chat-hf).

device_map="auto": Automatically chooses CPU or GPU based on availability.

model_kwargs: Optional settings for memory optimization on CUDA devices (torch_dtype=torch.float16, load_in_8bit=True)
"""

import torch

llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={"temperature": 0.0, "do_sample": False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name="meta-llama/Llama-2-7b-chat-hf",
    model_name="meta-llama/Llama-2-7b-chat-hf",
    device_map="auto",
    # uncomment this if using CUDA to reduce memory usage
    model_kwargs={"torch_dtype": torch.float16 , "load_in_8bit":True}
)

"""
1. Import necessary classes:

HuggingFaceEmbeddings (from langchain.embeddings.huggingface): Accesses embedding models from Hugging Face.
ServiceContext and LangchainEmbedding (from llama_index): Manage service configurations and create embeddings for indexing.
2. Instantiate LangchainEmbedding:

Wraps a HuggingFaceEmbeddings model to make it compatible with llama_index.
Specifies model_name="sentence-transformers/all-mpnet-base-v2": Uses a multi-purpose sentence transformer model for text embedding.
Key concepts:

Embeddings: Dense vectors that capture semantic meaning of text, enabling similarity comparisons and retrieval.
HuggingFaceEmbeddings: Provides access to various embedding models on Hugging Face.
LangchainEmbedding: Adapts embedding models for use within llama_index.
sentence-transformers/all-mpnet-base-v2: A versatile model trained on multi-lingual text for various semantic tasks."""

from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from llama_index import ServiceContext
from llama_index.embeddings import LangchainEmbedding

embed_model=LangchainEmbedding(
    HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2"))

service_context=ServiceContext.from_defaults(
    chunk_size=1024,
    llm=llm,
    embed_model=embed_model
)

service_context

index=VectorStoreIndex.from_documents(documents,service_context=service_context)

index

query_engine=index.as_query_engine()

response=query_engine.query("what is A residual dense vision transformer for medical image super-resolution with segmentation-based perceptual loss fine-tuning?")

print(response)

